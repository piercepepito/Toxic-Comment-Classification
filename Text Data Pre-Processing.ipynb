{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pierce\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")\n",
    "\n",
    "df = pd.concat([train, test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This dictionary came from Prashant Kikani's kernel in Kaggle, https://www.kaggle.com/prashantkikani/pooled-gru-with-preprocessing\n",
    "\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"R\" : \"Are\",\n",
    "    \"u\": \"you\",\n",
    "    \"U\" : \"You\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"Y\" : \"why\",\n",
    "    \"y\" : \"why\",\n",
    "    \"EU\" : \"European Union\",\n",
    "    \"US\" : \"United States\",\n",
    "    \"U.S.\" : \"United States\",\n",
    "    \"ur\" : \"your\"\n",
    "}\n",
    "\n",
    "df[\"comment_text\"] = df[\"comment_text\"].transform(lambda x: \" \".join([repl[word] if word in repl else word for word in \n",
    "                          re.split(\" \", x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word replacement\n",
    "#Removing html tags\n",
    "html_tags = re.compile(\"(http\\S+|www.\\S+)\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(html_tags, \"website\")\n",
    "\n",
    "#websites .com/.net/.org/.gov - prototype\n",
    "website = re.compile(\"([A-Za-z0-9]\\S+\\.com|[A-Za-z0-9]\\S+\\.net|[A-Za-z0-9]\\S+\\.gov|[A-Za-z0-9]\\S+\\.org)\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(website, \"website\")\n",
    "\n",
    "#wikipedia - rpototype (usure if this feature would be useful on LSTM networks)\n",
    "wiki = re.compile(\"wiki[a-zA-Z]\\S*\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(wiki, \"wikipedia\")\n",
    "\n",
    "# email address - prototype\n",
    "email = \"[a-zA-Z0-9]+@[a-zA-Z]{2,}\\.\\S+\\w\"\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(email, \"email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Noise Removal\n",
    "#Removing IP Address\n",
    "ip_address = re.compile(\"[0-9]{2,5}.[0-9]{2,5}.[0-9]{2,5}.[0-9]{2,5}\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(ip_address, \"\")\n",
    "\n",
    "#hashtag removal\n",
    "hashtag = re.compile(\"#\\S+\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(hashtag, \"\")\n",
    "\n",
    "#time stamp removal\n",
    "timestamp = re.compile(\"([0-9]{2}:[0-9]{2}.+UTC\\S+|[0-9]{2}:[0-9]{2}.*|[A-Za-z]{3,10}\\s[0-9]{1,4}.*utc\\S)\", re.I)\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(timestamp, \"\")\n",
    "\n",
    "#article id\n",
    "article_id = re.compile(\"\\d:\\d\\d\\s{0,5}$\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(article_id, \"\")\n",
    "\n",
    "#username\n",
    "username = re.compile(\"\\[\\[User(.*)\\|\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(username, \"\")\n",
    "\n",
    "#added ==\n",
    "equal = re.compile(\"==\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(equal, \"\")\n",
    "\n",
    "#filename extensions (jpeg, png) and miscellaneous\n",
    "misc = re.compile(\"[A-Za-z]+:\\S+\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(misc, \"\")\n",
    "\n",
    "#removing mutiple whitespaces\n",
    "whitespace = re.compile(\"[ ]{2,}\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(whitespace, \" \")\n",
    "\n",
    "#noise (alphabet characters followed by numeric character)- prototype\n",
    "alpha_numeric = re.compile(\"[0-9]{5,}[a-zA-Z]{3,}\\S+\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(alpha_numeric, \"\")\n",
    "\n",
    "#noise hold little information - parenthesis - prototype\n",
    "paren = re.compile(\"\\((.{,10})\\)|[a-zA-Z0-9]+@\\S+|\\([0-9]{1,}\\.[0-9]{1,}\\.[0-9]{1,}\\.[0-9]{1,}\\)\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(paren, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# single letter words\n",
    "\n",
    "#dimensions\n",
    "x_by = re.compile(\"(x|X)(?=\\s[0-9]{1,})\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(x_by, \"by\")\n",
    "\n",
    "#single letter but not part of words in english dictionary\n",
    "single = re.compile(\"\\s[b-hj-zB-HJ-Z]\\s\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(single, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separate words joind by (/, -)\n",
    "# /\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.split(\"(/)\").transform(lambda x: \n",
    "                                                                        \" \".join([re.sub(\"/\", \"or\", y) for y in x]))\n",
    "\n",
    "# - \n",
    "dash = re.compile(\"(-)|(_)\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(dash, \" \")\n",
    "\n",
    "#for stastical methods (tf-idf)\n",
    "# - removing punctuations\n",
    "punctuation = re.compile(\"\\W+\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(punctuation, \" \")\n",
    "\n",
    "#normalize by lower case entire corpus\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stem all words in the comments, which will be used for tf-idf\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "df[\"comment_text\"] = df[\"comment_text\"].transform(lambda x: \" \".join([stemmer.stem(word) for word in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df.iloc[:len(train), :]\n",
    "test = df.iloc[len(train):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"train_clean.csv\", index = False)\n",
    "test.to_csv(\"test_clean.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
