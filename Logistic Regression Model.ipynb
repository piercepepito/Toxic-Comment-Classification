{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Toxic Comments\n",
    "\n",
    "After performing EDA and preprocessing, it's time to make a classification model. In this section of the project, the text data will be transformed into their respective numerical representation, by tf-idf, and a logistic regression model is built on all 6 classification tags.\n",
    "\n",
    "Note: This machine learning model is originally by [Bojan Tunguz](https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams), which can be found in Kaggle. I've tweaked the hyper parameters from the classifier as well as the TfidfVectorizer to optimize ROC-AUC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed/train_clean.csv\", encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset = [\"comment_text\"], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(ngram_range = (1, 4),\n",
    "                                  stop_words = \"english\",\n",
    "                                  max_features = 20000,\n",
    "                                  sublinear_tf = True,\n",
    "                                  analyzer = \"word\"\n",
    "                )\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(ngram_range = (2, 5),\n",
    "                                stop_words = \"english\",\n",
    "                                max_features = 20000,\n",
    "                                sublinear_tf = True,\n",
    "                                analyzer = \"char\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf has been used two times, one for carrying information at a character and on a word level. Tf-idf is used here since the conventional count encoding, does not penalize frequently used words. The comments have been vectorized in a charcter level it is [recommended](https://developers.google.com/machine-learning/guides/text-classification/step-3) to use whenever there's a lot of wrongly spelled words in the corpus. The purpose of the word vectorizer is to find out the sequence of words which are most likely to come together. Lastly, the use of 20 000 features on both tfidf vectorizer is because, the same study made by Google, have shown that a lot of the datasets they've used have peaked their performance at 20 000.\n",
    "\n",
    "The use of ngram range of 1 to 4 in the word vectorizer, came from a [paper](https://arxiv.org/pdf/1708.08123.pdf) made by Vedhera, Grossman, and Cormack, and in their study, whenever using Logistic Regression for Tf-idf, they got the highest scoring from this ngram range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_word = word_vectorizer.fit_transform(df[\"comment_text\"])\n",
    "train_char = char_vectorizer.fit_transform(df[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = hstack([train_char, train_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hstack command has been used to combine both the character and word level feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class identity_hate has mean of 0.9824 and std of 0.0010\n",
      "CV score for class insult has mean of 0.9821 and std of 0.0006\n",
      "CV score for class obscene has mean of 0.9896 and std of 0.0010\n",
      "CV score for class severe_toxic has mean of 0.9878 and std of 0.0014\n",
      "CV score for class threat has mean of 0.9894 and std of 0.0024\n",
      "CV score for class toxic has mean of 0.9767 and std of 0.0010\n",
      "ROC_AUC mean: 0.9846678156975965\n",
      "ROC_AUC std: 0.004881091193700803\n"
     ]
    }
   ],
   "source": [
    "#initialization\n",
    "classes = [\"identity_hate\", \"insult\", \"obscene\", \"severe_toxic\", \"threat\", \"toxic\"]\n",
    "\n",
    "scores = []\n",
    "\n",
    "#fit model\n",
    "for class_category in classes:\n",
    "    train_target = df[class_category]\n",
    "    classifier = LogisticRegression(random_state = 6)\n",
    "\n",
    "    cv_score = cross_val_score(classifier, train, train_target, cv = 3, scoring='roc_auc')\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} has mean of {:.4f} and std of {:.4f}'.format(class_category, np.mean(cv_score), np.std(cv_score)))\n",
    "\n",
    "print(\"ROC_AUC mean: {}\".format(np.mean(scores)))\n",
    "print(\"ROC_AUC std: {}\".format(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average ROC-AUC for the Logistic Regression Model, has over 98.47%. Also all Logistic Regression models for the 6 classification tags have been consistent ranging from 97.7% up to 98.96%, with very little stand deviations. \n",
    "\n",
    "I've used Naive Bayes, as well as a bidirectional GRU but both of them have very little to add. By using Naive Bayes, the ROC was a lot lower compared to Logistic Regression, and using GRU's, there's very little increase in performance. Another reason for not choosing GRU over Logistic Regression is that it is a lot easier to maintain and the training time, when using Logistic Regression, is a lot faster comapred to GRU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
